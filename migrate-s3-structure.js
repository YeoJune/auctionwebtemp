// migrate-s3-structure.js
require("dotenv").config();
const {
  S3Client,
  CopyObjectCommand,
  DeleteObjectCommand,
  ListObjectsV2Command,
} = require("@aws-sdk/client-s3");
const { pool } = require("./utils/DB");
const fs = require("fs").promises;

class S3StructureMigration {
  constructor() {
    this.s3Client = new S3Client({
      region: process.env.AWS_REGION || "ap-northeast-2",
      credentials: {
        accessKeyId: process.env.AWS_ACCESS_KEY_ID,
        secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY,
      },
    });

    this.bucketName = process.env.S3_BUCKET_NAME || "casa-images";
    this.cloudFrontDomain = process.env.CLOUDFRONT_DOMAIN;
    this.oldPrefix = "values/";
    this.newPrefix = "values/";

    // DB ì¿¼ë¦¬ ê²°ê³¼ ìºì‹œ (íŒŒì¼ëª… â†’ ë‚ ì§œ)
    this.dateCache = new Map();

    // URL ë§¤í•‘ íŒŒì¼ (DB ì—…ë°ì´íŠ¸ìš©)
    this.urlMappingFile = "./s3-migration-mappings.json";
    this.urlMappings = [];

    this.stats = {
      totalFiles: 0,
      processed: 0,
      success: 0,
      failed: 0,
      skipped: 0,
      dbCacheHits: 0,
      dbUpdated: 0,
      startTime: null,
    };
  }

  /**
   * DBì—ì„œ ëª¨ë“  ì´ë¯¸ì§€ ë‚ ì§œ ì •ë³´ ì‚¬ì „ ë¡œë”© (ë©”ëª¨ë¦¬ ìºì‹±)
   */
  async preloadDateCache() {
    console.log("[Migration] Preloading date cache from DB...");
    const startTime = Date.now();

    try {
      const [rows] = await pool.query(
        `SELECT image, additional_images, scheduled_date 
         FROM values_items 
         WHERE scheduled_date IS NOT NULL`
      );

      for (const row of rows) {
        // image ì»¬ëŸ¼
        if (row.image) {
          const fileName = row.image.split("/").pop();
          if (!this.dateCache.has(fileName)) {
            this.dateCache.set(fileName, row.scheduled_date);
          }
        }

        // additional_images JSON ë°°ì—´
        if (row.additional_images) {
          try {
            const additionalImages = JSON.parse(row.additional_images);
            for (const imgUrl of additionalImages) {
              const fileName = imgUrl.split("/").pop();
              if (!this.dateCache.has(fileName)) {
                this.dateCache.set(fileName, row.scheduled_date);
              }
            }
          } catch (e) {
            // JSON íŒŒì‹± ì‹¤íŒ¨ ë¬´ì‹œ
          }
        }
      }

      const elapsed = ((Date.now() - startTime) / 1000).toFixed(2);
      console.log(
        `[Migration] Cached ${this.dateCache.size} file dates in ${elapsed}s`
      );
    } catch (error) {
      console.error("[Migration] Failed to preload date cache:", error.message);
      throw error;
    }
  }

  /**
   * ì´ë¯¸ì§€ í•˜ìœ„ í´ë” ê²½ë¡œ ìƒì„±
   */
  getImageSubFolder(scheduledDate, fileName) {
    let yearMonth;

    if (scheduledDate) {
      try {
        const date = new Date(scheduledDate);
        if (!isNaN(date.getTime())) {
          yearMonth = date.toISOString().slice(0, 7);
        }
      } catch (e) {
        yearMonth = "legacy";
      }
    }

    if (!yearMonth) {
      yearMonth = "legacy";
    }

    const firstChar = fileName.charAt(0).toLowerCase();
    return `${yearMonth}/${firstChar}`;
  }

  /**
   * S3ì—ì„œ values/ í´ë”ì˜ ëª¨ë“  íŒŒì¼ ë¦¬ìŠ¤íŠ¸
   */
  async listAllS3Files() {
    const files = [];
    let continuationToken = null;

    console.log("[Migration] Listing all S3 files...");

    do {
      const command = new ListObjectsV2Command({
        Bucket: this.bucketName,
        Prefix: this.oldPrefix,
        ContinuationToken: continuationToken,
        MaxKeys: 1000,
      });

      const response = await this.s3Client.send(command);

      if (response.Contents) {
        // í•˜ìœ„ í´ë” êµ¬ì¡°ê°€ ì•„ë‹Œ íŒŒì¼ë§Œ (values/xxx.webp í˜•íƒœ)
        const flatFiles = response.Contents.filter((item) => {
          const relativePath = item.Key.replace(this.oldPrefix, "");
          // ìŠ¬ë˜ì‹œê°€ ì—†ìœ¼ë©´ í‰ë©´ êµ¬ì¡°
          return !relativePath.includes("/");
        });

        files.push(...flatFiles.map((item) => item.Key));
      }

      continuationToken = response.NextContinuationToken;

      if (files.length % 50000 === 0 && files.length > 0) {
        console.log(`[Migration] Found ${files.length} files so far...`);
      }
    } while (continuationToken);

    console.log(`[Migration] Total files to migrate: ${files.length}`);
    return files;
  }

  /**
   * ìºì‹œì—ì„œ scheduled_date ì¡°íšŒ (DB ì¿¼ë¦¬ ì œê±°)
   */
  getItemDateFromCache(fileName) {
    const cleanFileName = fileName.replace(this.oldPrefix, "");

    if (this.dateCache.has(cleanFileName)) {
      this.stats.dbCacheHits++;
      return this.dateCache.get(cleanFileName);
    }

    return null; // legacy í´ë”ë¡œ ì´ë™
  }

  /**
   * S3 íŒŒì¼ ì´ë™ (ì„œë²„ ì¸¡ ë³µì‚¬)
   */
  async moveS3File(oldKey, newKey) {
    try {
      // 1. ë³µì‚¬
      await this.s3Client.send(
        new CopyObjectCommand({
          Bucket: this.bucketName,
          CopySource: `${this.bucketName}/${oldKey}`,
          Key: newKey,
          MetadataDirective: "COPY",
          CacheControl: "max-age=31536000, immutable",
        })
      );

      // 2. ì›ë³¸ ì‚­ì œ
      await this.s3Client.send(
        new DeleteObjectCommand({
          Bucket: this.bucketName,
          Key: oldKey,
        })
      );

      return true;
    } catch (error) {
      console.error(`Failed to move ${oldKey} to ${newKey}:`, error.message);
      return false;
    }
  }

  /**
   * URL ë§¤í•‘ íŒŒì¼ì— ì €ì¥ (DB ì—…ë°ì´íŠ¸ëŠ” ë‚˜ì¤‘ì— ë³„ë„ ì‹¤í–‰)
   */
  async saveUrlMapping(oldUrl, newUrl) {
    this.urlMappings.push({ oldUrl, newUrl });

    // 10000ê°œë§ˆë‹¤ íŒŒì¼ì— ì €ì¥ (ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œìš° ë°©ì§€)
    if (this.urlMappings.length >= 10000) {
      await this.flushUrlMappings();
    }
  }

  /**
   * ë©”ëª¨ë¦¬ì˜ URL ë§¤í•‘ì„ íŒŒì¼ì— ê¸°ë¡
   */
  async flushUrlMappings() {
    if (this.urlMappings.length === 0) return;

    try {
      const jsonData = this.urlMappings
        .map((m) => JSON.stringify(m))
        .join("\n");
      await fs.appendFile(this.urlMappingFile, jsonData + "\n");
      this.urlMappings = [];
    } catch (error) {
      console.error("Failed to flush URL mappings:", error.message);
    }
  }

  /**
   * DB ë°°ì¹˜ ì—…ë°ì´íŠ¸ (íŠ¸ëœì­ì…˜) - ìµœì í™”
   */
  async updateDBBatch(mappings) {
    if (mappings.length === 0) return 0;

    const conn = await pool.getConnection();
    let updatedCount = 0;

    try {
      await conn.beginTransaction();

      // URL ë§µ ìƒì„± (old â†’ new)
      const urlMap = new Map(mappings.map((m) => [m.oldUrl, m.newUrl]));
      const oldUrls = Array.from(urlMap.keys());

      // CASE WHENìœ¼ë¡œ í•œ ë²ˆì— ì—…ë°ì´íŠ¸ (í›¨ì”¬ ë¹ ë¦„)
      if (oldUrls.length > 0) {
        // image ì»¬ëŸ¼ ì¼ê´„ ì—…ë°ì´íŠ¸
        const imageCases = oldUrls.map((oldUrl) => `WHEN ? THEN ?`).join(" ");
        const imagePlaceholders = oldUrls.flatMap((oldUrl) => [
          oldUrl,
          urlMap.get(oldUrl),
        ]);
        const imageWhereIn = oldUrls.map(() => "?").join(",");

        const [imageResult] = await conn.query(
          `UPDATE values_items 
           SET image = CASE image ${imageCases} END
           WHERE image IN (${imageWhereIn})`,
          [...imagePlaceholders, ...oldUrls]
        );

        updatedCount += imageResult.affectedRows;

        // additional_images ì¼ê´„ ì—…ë°ì´íŠ¸ (REPLACE)
        for (const { oldUrl, newUrl } of mappings) {
          const [additionalResult] = await conn.query(
            `UPDATE values_items 
             SET additional_images = REPLACE(additional_images, ?, ?)
             WHERE additional_images LIKE ?`,
            [oldUrl, newUrl, `%${oldUrl}%`]
          );

          if (additionalResult.affectedRows > 0) {
            updatedCount += additionalResult.affectedRows;
          }
        }
      }

      await conn.commit();
      return updatedCount;
    } catch (error) {
      await conn.rollback();
      console.error(`Batch DB update failed:`, error.message);
      return 0;
    } finally {
      conn.release();
    }
  }

  /**
   * ë‹¨ì¼ íŒŒì¼ ë§ˆì´ê·¸ë ˆì´ì…˜ (S3ë§Œ ì²˜ë¦¬)
   */
  async migrateFile(s3Key) {
    const fileName = s3Key.replace(this.oldPrefix, "");

    // ì´ë¯¸ í•˜ìœ„ í´ë” êµ¬ì¡°ë©´ ìŠ¤í‚µ
    if (fileName.includes("/")) {
      this.stats.skipped++;
      return null;
    }

    // ìºì‹œì—ì„œ scheduled_date ì¡°íšŒ (DB ì¿¼ë¦¬ ì—†ìŒ)
    const scheduledDate = this.getItemDateFromCache(fileName);

    // ìƒˆ S3 í‚¤ ìƒì„±
    const subFolder = this.getImageSubFolder(scheduledDate, fileName);
    const newKey = `${this.newPrefix}${subFolder}/${fileName}`;

    // S3 íŒŒì¼ ì´ë™
    const moveSuccess = await this.moveS3File(s3Key, newKey);

    if (moveSuccess) {
      this.stats.success++;

      // URL ë§¤í•‘ ì €ì¥ (íŒŒì¼ë¡œ)
      await this.saveUrlMapping(
        `https://${this.cloudFrontDomain}/${s3Key}`,
        `https://${this.cloudFrontDomain}/${newKey}`
      );

      return true;
    } else {
      this.stats.failed++;
      return null;
    }
  }

  /**
   * ë°°ì¹˜ ì²˜ë¦¬ (ìˆœìˆ˜ S3 ì‘ì—…ë§Œ - ìµœëŒ€ ë³‘ë ¬)
   */
  async migrateBatch(files, batchSize = 500) {
    for (let i = 0; i < files.length; i += batchSize) {
      const batch = files.slice(i, i + batchSize);

      // S3 íŒŒì¼ ì´ë™ë§Œ ë³‘ë ¬ ì‹¤í–‰ (DB ì‘ì—… ì—†ìŒ)
      await Promise.all(batch.map((file) => this.migrateFile(file)));

      this.stats.processed += batch.length;

      // ì§„í–‰ë¥  ì¶œë ¥ (5000ê°œë§ˆë‹¤)
      if (
        this.stats.processed % 5000 === 0 ||
        this.stats.processed === files.length
      ) {
        this.logProgress();
      }

      // S3 API rate limit ê³ ë ¤ (10000ê°œë§ˆë‹¤ ì§§ì€ ëŒ€ê¸°)
      if (i % 10000 === 0 && i > 0) {
        await this.sleep(100);
      }
    }

    // ë‚¨ì€ URL ë§¤í•‘ í”ŒëŸ¬ì‹œ
    await this.flushUrlMappings();
  }

  /**
   * ë§¤í•‘ íŒŒì¼ì—ì„œ DB ì—…ë°ì´íŠ¸ (í”„ë¡œê·¸ë˜ë° ë°©ì‹ - ì´ˆê³ ì†)
   */
  async updateDBFromMappings() {
    const readline = require("readline");
    const { createReadStream } = require("fs");

    try {
      // íŒŒì¼ ì¡´ì¬ í™•ì¸
      await fs.access(this.urlMappingFile);

      console.log("[DB Update] Loading all items from DB...");
      const [items] = await pool.query(
        `SELECT id, image, additional_images FROM values_items`
      );
      console.log(`[DB Update] Loaded ${items.length} items`);

      // URL â†’ ìƒˆ URL ë§µ ìƒì„±
      console.log("[DB Update] Loading mappings...");
      const urlMap = new Map();
      const stream = createReadStream(this.urlMappingFile);
      const rl = readline.createInterface({ input: stream });

      for await (const line of rl) {
        if (!line.trim()) continue;
        try {
          const { oldUrl, newUrl } = JSON.parse(line);
          urlMap.set(oldUrl, newUrl);
        } catch (e) {}
      }
      console.log(`[DB Update] Loaded ${urlMap.size} mappings`);

      // ë©”ëª¨ë¦¬ì—ì„œ ë§¤ì¹­
      console.log("[DB Update] Matching in memory...");
      const updates = [];
      let processedCount = 0;

      for (const item of items) {
        let changed = false;
        let newImage = item.image;
        let newAdditional = item.additional_images;

        // image ì»¬ëŸ¼ ë§¤ì¹­
        if (item.image && urlMap.has(item.image)) {
          newImage = urlMap.get(item.image);
          changed = true;
        }

        // additional_images ë§¤ì¹­
        if (item.additional_images) {
          try {
            const additionalImages = JSON.parse(item.additional_images);
            const updatedImages = additionalImages.map((url) =>
              urlMap.has(url) ? urlMap.get(url) : url
            );

            if (
              JSON.stringify(additionalImages) !== JSON.stringify(updatedImages)
            ) {
              newAdditional = JSON.stringify(updatedImages);
              changed = true;
            }
          } catch (e) {}
        }

        if (changed) {
          updates.push({
            id: item.id,
            image: newImage,
            additional_images: newAdditional,
          });
        }

        processedCount++;
        if (processedCount % 10000 === 0) {
          console.log(
            `[DB Update] Processed ${processedCount}/${items.length} items, ${updates.length} changes found`
          );
        }
      }

      console.log(`[DB Update] Found ${updates.length} items to update`);

      // ë°°ì¹˜ ì—…ë°ì´íŠ¸
      console.log("[DB Update] Writing to DB...");
      const batchSize = 500;
      for (let i = 0; i < updates.length; i += batchSize) {
        const batch = updates.slice(i, i + batchSize);

        const conn = await pool.getConnection();
        try {
          await conn.beginTransaction();

          for (const { id, image, additional_images } of batch) {
            await conn.query(
              `UPDATE values_items SET image = ?, additional_images = ? WHERE id = ?`,
              [image, additional_images, id]
            );
          }

          await conn.commit();
          this.stats.dbUpdated += batch.length;
        } catch (error) {
          await conn.rollback();
          console.error(`Batch update failed:`, error.message);
        } finally {
          conn.release();
        }

        if ((i + batchSize) % 5000 === 0) {
          console.log(
            `[DB Update] ${this.stats.dbUpdated}/${updates.length} rows updated`
          );
        }
      }

      console.log(`[DB Update] Complete: ${this.stats.dbUpdated} rows updated`);
    } catch (error) {
      if (error.code === "ENOENT") {
        console.error(`\nâŒ Mapping file not found: ${this.urlMappingFile}`);
        console.error("   Run S3 migration first without --update-db flag");
      } else {
        console.error("[DB Update] Fatal error:", error);
      }
      throw error;
    }
  }

  /**
   * ë©”ì¸ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤í–‰
   */
  async migrate(options = {}) {
    const { updateDB = false, dbOnly = false } = options;

    this.stats.startTime = Date.now();
    console.log(
      `\n[S3 Structure Migration] Starting at ${new Date().toISOString()}`
    );
    console.log("=".repeat(60));

    try {
      // DBë§Œ ì—…ë°ì´íŠ¸ ëª¨ë“œ (S3 ìŠ¤í‚µ)
      if (dbOnly) {
        console.log("[Migration] DB-ONLY MODE: Skipping S3 migration");
        console.log(
          "[Migration] Starting DB update from existing mapping file..."
        );
        await this.updateDBFromMappings();
        return this.getFinalStats();
      }

      // 0. ê¸°ì¡´ ë§¤í•‘ íŒŒì¼ ì‚­ì œ (S3 ì¬ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œ)
      try {
        await fs.unlink(this.urlMappingFile);
      } catch (e) {
        // íŒŒì¼ ì—†ìœ¼ë©´ ë¬´ì‹œ
      }

      // 1. DB ë‚ ì§œ ìºì‹œ ì‚¬ì „ ë¡œë”© (í•œ ë²ˆë§Œ)
      await this.preloadDateCache();

      // 2. S3 íŒŒì¼ ë¦¬ìŠ¤íŠ¸
      const files = await this.listAllS3Files();
      this.stats.totalFiles = files.length;

      if (files.length === 0) {
        console.log(
          "[Migration] No flat files to migrate (already structured)"
        );

        // S3ëŠ” ì™„ë£Œëì§€ë§Œ DB ì—…ë°ì´íŠ¸ í•„ìš”í•œ ê²½ìš°
        if (updateDB) {
          console.log("[Migration] Checking for existing mapping file...");
          try {
            await fs.access(this.urlMappingFile);
            console.log("[Migration] Found mapping file, updating DB...");
            await this.updateDBFromMappings();
          } catch (e) {
            console.log(
              "[Migration] No mapping file found. S3 migration already complete."
            );
          }
        }

        return this.getFinalStats();
      }

      // 3. S3 ë°°ì¹˜ ì²˜ë¦¬ (ë³‘ë ¬ ìµœëŒ€í™”)
      console.log("[Migration] Starting S3 file migration (parallel)...");
      await this.migrateBatch(files);

      // 4. DB ì—…ë°ì´íŠ¸ (ì˜µì…˜)
      if (updateDB) {
        console.log("\n[Migration] Starting DB update...");
        await this.updateDBFromMappings();
      }

      // 5. ìµœì¢… í†µê³„
      const stats = this.getFinalStats();

      if (!updateDB) {
        console.log(`\nğŸ“ URL mappings saved to: ${this.urlMappingFile}`);
        console.log(
          `   To update DB, run: node migrate-s3-structure.js --db-only`
        );
      }

      return stats;
    } catch (error) {
      console.error("[Migration] Fatal error:", error);
      throw error;
    }
  }

  /**
   * ì§„í–‰ë¥  ì¶œë ¥
   */
  logProgress() {
    const elapsed = Date.now() - this.stats.startTime;
    const rate = this.stats.processed / (elapsed / 1000);
    const remaining = this.stats.totalFiles - this.stats.processed;
    const eta = rate > 0 ? Math.round(remaining / rate / 60) : 0;
    const cacheHitRate =
      this.stats.processed > 0
        ? ((this.stats.dbCacheHits / this.stats.processed) * 100).toFixed(1)
        : 0;

    console.log(
      `[S3 Migration] ${this.stats.processed}/${this.stats.totalFiles} | ` +
        `âœ“${this.stats.success} âœ—${this.stats.failed} âŠ˜${this.stats.skipped} | ` +
        `Cache: ${cacheHitRate}% | ${rate.toFixed(1)}/s | ETA: ${eta}min`
    );
  }

  /**
   * ìµœì¢… í†µê³„
   */
  getFinalStats() {
    const duration = Date.now() - this.stats.startTime;
    const cacheHitRate =
      this.stats.processed > 0
        ? ((this.stats.dbCacheHits / this.stats.processed) * 100).toFixed(1)
        : 0;

    return {
      totalFiles: this.stats.totalFiles,
      processed: this.stats.processed,
      success: this.stats.success,
      failed: this.stats.failed,
      skipped: this.stats.skipped,
      dbUpdated: this.stats.dbUpdated,
      cacheHitRate: `${cacheHitRate}%`,
      duration: this.formatDuration(duration),
      avgRate:
        this.stats.processed > 0
          ? (this.stats.processed / (duration / 1000)).toFixed(2)
          : 0,
    };
  }

  /**
   * ì‹œê°„ í¬ë§·
   */
  formatDuration(ms) {
    const seconds = Math.floor(ms / 1000);
    const minutes = Math.floor(seconds / 60);
    const hours = Math.floor(minutes / 60);

    const remainingMinutes = minutes % 60;
    const remainingSeconds = seconds % 60;

    if (hours > 0) {
      return `${hours}h ${remainingMinutes}m ${remainingSeconds}s`;
    } else if (minutes > 0) {
      return `${minutes}m ${remainingSeconds}s`;
    } else {
      return `${seconds}s`;
    }
  }

  /**
   * ëŒ€ê¸° í•¨ìˆ˜
   */
  sleep(ms) {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }
}

/**
 * ë©”ì¸ ì‹¤í–‰
 */
async function main() {
  const migration = new S3StructureMigration();

  // ì»¤ë§¨ë“œë¼ì¸ ì˜µì…˜ íŒŒì‹±
  const args = process.argv.slice(2);
  const updateDB = args.includes("--update-db");
  const dbOnly = args.includes("--db-only");

  try {
    console.log("\n" + "=".repeat(60));
    console.log("S3 Structure Migration Tool");
    console.log("=".repeat(60));
    console.log(
      "This will reorganize values/ images into date-based subfolders"
    );
    console.log("Example: values/xxx.webp â†’ values/2025-01/x/xxx.webp");

    if (dbOnly) {
      console.log(
        "\nğŸ’¾ DB-ONLY MODE: Will only update database from mapping file"
      );
    } else if (updateDB) {
      console.log("\nğŸ”„ FULL MODE: S3 migration + DB update");
    } else {
      console.log("\nğŸ“¦ S3 ONLY MODE: DB update with --db-only later");
    }
    console.log("=".repeat(60) + "\n");

    const stats = await migration.migrate({ updateDB, dbOnly });

    console.log("\n" + "=".repeat(60));
    console.log("[Migration] Final Statistics");
    console.log("=".repeat(60));
    console.log(`Total Files: ${stats.totalFiles}`);
    console.log(`Processed: ${stats.processed}`);
    console.log(`Successfully Migrated: ${stats.success}`);
    console.log(`Failed: ${stats.failed}`);
    console.log(`Skipped (already structured): ${stats.skipped}`);
    if (updateDB || dbOnly) {
      console.log(`DB Updated: ${stats.dbUpdated} rows`);
    }
    console.log(`DB Cache Hit Rate: ${stats.cacheHitRate}`);
    console.log(`Duration: ${stats.duration}`);
    console.log(`Average Rate: ${stats.avgRate} files/sec`);
    console.log("=".repeat(60));

    if (stats.failed > 0) {
      console.log("\nâš ï¸  Some files failed to migrate. Check logs above.");
    } else {
      console.log("\nâœ… Migration completed successfully!");
    }

    process.exit(0);
  } catch (error) {
    console.error("\nâŒ Migration failed:", error);
    process.exit(1);
  }
}

// ì§ì ‘ ì‹¤í–‰ ì‹œ
if (require.main === module) {
  main();
}

module.exports = { S3StructureMigration };
