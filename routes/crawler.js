// routes/crawler.js
const express = require("express");
const router = express.Router();
const {
  ecoAucCrawler,
  ecoAucValueCrawler,
  brandAucCrawler,
  brandAucValueCrawler,
  starAucCrawler,
  starAucValueCrawler,
  mekikiAucCrawler,
  mekikiAucValueCrawler,
  penguinAucCrawler,
} = require("../crawlers/index");
const DBManager = require("../utils/DBManager");
const { pool } = require("../utils/DB");
const cron = require("node-cron");
const { getAdminSettings } = require("../utils/adminDB");
const { syncAllData } = require("../utils/dataUtils");
const dotenv = require("dotenv");
const socketIO = require("socket.io");
const { sendHigherBidAlerts } = require("../utils/message");
const { ValuesImageMigration } = require("../utils/s3Migration");
const { processImagesInChunks } = require("../utils/processImage");
const esManager = require("../utils/elasticsearch");
const {
  refundDeposit,
  refundLimit,
  getBidDeductAmount,
} = require("../utils/deposit");

dotenv.config();

// Elasticsearch ì¬ì¸ë±ì‹± ë°°ì¹˜ í¬ê¸°
const ES_REINDEX_BATCH_SIZE = 10000;

let isCrawling = false;
let isValueCrawling = false;
let isUpdateCrawling = false;
let isUpdateCrawlingWithId = false;

const isAdmin = (req, res, next) => {
  if (req.session.user && req.session.user.login_id === "admin") {
    next();
  } else {
    res.status(403).json({ message: "Access denied. Admin only." });
  }
};

class AdaptiveScheduler {
  constructor(baseInterval) {
    this.base = baseInterval;
    this.min = baseInterval;
    this.max = baseInterval * 4;
    this.current = baseInterval * 2;
    this.noChangeCount = 0;
  }

  next(changedCount) {
    if (changedCount === 0) {
      // Additive Increase with log acceleration
      this.noChangeCount++;
      const increment = this.base * Math.log2(this.noChangeCount + 1) * 0.1;
      this.current = Math.min(this.current + increment, this.max);
    } else {
      // Multiplicative Decrease
      this.noChangeCount = 0;
      const changeRate = Math.min(changedCount / 10, 1.0);
      const decreaseRate = 0.5 + 0.4 * changeRate;
      this.current = Math.max(this.current * (1 - decreaseRate), this.min);
    }
    return Math.round(this.current);
  }

  reset() {
    this.current = this.base;
    this.noChangeCount = 0;
  }

  getStatus() {
    return {
      current: Math.round(this.current),
      min: this.min,
      max: this.max,
      noChangeCount: this.noChangeCount,
    };
  }
}

// ê²½ë§¤ì‚¬ ì„¤ì •
const AUCTION_CONFIG = {
  1: {
    name: "EcoAuc",
    crawler: ecoAucCrawler,
    valueCrawler: ecoAucValueCrawler,
    enabled: true,
    updateInterval: parseInt(process.env.UPDATE_INTERVAL, 10) || 40,
    updateWithIdInterval: parseInt(process.env.UPDATE_INTERVAL_ID, 10) || 10,
  },
  2: {
    name: "BrandAuc",
    crawler: brandAucCrawler,
    valueCrawler: brandAucValueCrawler,
    enabled: true,
    updateInterval: parseInt(process.env.UPDATE_INTERVAL, 10) || 40,
    updateWithIdInterval: parseInt(process.env.UPDATE_INTERVAL_ID, 10) || 10,
  },
  3: {
    name: "StarAuc",
    crawler: starAucCrawler,
    valueCrawler: starAucValueCrawler,
    enabled: false,
    updateInterval: parseInt(process.env.UPDATE_INTERVAL, 10) || 40,
    updateWithIdInterval: parseInt(process.env.UPDATE_INTERVAL_ID, 10) || 10,
  },
  4: {
    name: "MekikiAuc",
    crawler: mekikiAucCrawler,
    valueCrawler: mekikiAucValueCrawler,
    enabled: true,
    updateInterval: 0,
    updateWithIdInterval: 0,
  },
  5: {
    name: "PenguinAuc",
    crawler: penguinAucCrawler,
    valueCrawler: null,
    enabled: true,
    updateInterval: parseInt(process.env.UPDATE_INTERVAL, 10) || 40,
    updateWithIdInterval: parseInt(process.env.UPDATE_INTERVAL_ID, 10) || 10,
  },
};

// ê²½ë§¤ì‚¬ë³„ ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
const updateSchedulers = {};
const updateWithIdSchedulers = {};
const crawlingStatus = {
  update: {},
  updateWithId: {},
};

Object.entries(AUCTION_CONFIG).forEach(([aucNum, config]) => {
  if (config.enabled) {
    // updateIntervalì´ 0ì´ ì•„ë‹ ë•Œë§Œ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    if (config.updateInterval > 0) {
      updateSchedulers[aucNum] = new AdaptiveScheduler(config.updateInterval);
      crawlingStatus.update[aucNum] = false;
    }
    // updateWithIdIntervalì´ 0ì´ ì•„ë‹ ë•Œë§Œ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±
    if (config.updateWithIdInterval > 0) {
      updateWithIdSchedulers[aucNum] = new AdaptiveScheduler(
        config.updateWithIdInterval,
      );
      crawlingStatus.updateWithId[aucNum] = false;
    }
  }
});

async function loginAll() {
  const crawlers = [];

  // Configì—ì„œ í™œì„±í™”ëœ ëª¨ë“  í¬ë¡¤ëŸ¬ ìˆ˜ì§‘
  Object.values(AUCTION_CONFIG).forEach((config) => {
    if (config.enabled) {
      if (config.crawler) crawlers.push(config.crawler);
      if (config.valueCrawler) crawlers.push(config.valueCrawler);
    }
  });

  await Promise.all(crawlers.map((crawler) => crawler.login()));
}

// Elasticsearch ì „ì²´ ì¬ì¸ë±ì‹± í•¨ìˆ˜
async function reindexElasticsearch(tableName) {
  try {
    if (!esManager.isHealthy()) {
      console.log("ES not available, skipping reindexing");
      return;
    }

    console.log(`\nğŸ”„ Starting Elasticsearch reindexing for ${tableName}...`);

    // 1. ì¸ë±ìŠ¤ ì‚­ì œ
    try {
      await esManager.deleteIndex(tableName);
      console.log(`âœ“ Deleted index: ${tableName}`);
    } catch (error) {
      console.log(`â†’ Index ${tableName} does not exist or already deleted`);
    }

    // 2. ì¸ë±ìŠ¤ ì¬ìƒì„±
    await esManager.createIndex(tableName);
    console.log(`âœ“ Created index: ${tableName}`);

    // 3. ë°ì´í„° ì¡°íšŒ
    const whereClause =
      tableName === "crawled_items"
        ? "WHERE is_enabled = 1 AND title IS NOT NULL"
        : "WHERE title IS NOT NULL";
    const [items] = await pool.query(`
      SELECT 
        item_id, title, brand, category, 
        auc_num, scheduled_date
      FROM ${tableName}
      ${whereClause}
    `);

    if (items.length === 0) {
      console.log(`No items to index in ${tableName}`);
      return;
    }

    console.log(`Found ${items.length} items to reindex`);

    // 4. ë°°ì¹˜ë¡œ ì¸ë±ì‹±
    let totalIndexed = 0;
    let totalErrors = 0;

    for (let i = 0; i < items.length; i += ES_REINDEX_BATCH_SIZE) {
      const batch = items.slice(i, i + ES_REINDEX_BATCH_SIZE);
      const batchNum = Math.floor(i / ES_REINDEX_BATCH_SIZE) + 1;
      const totalBatches = Math.ceil(items.length / ES_REINDEX_BATCH_SIZE);

      const result = await esManager.bulkIndex(tableName, batch);
      totalIndexed += result.indexed;
      totalErrors += result.errors;

      console.log(
        `  Batch ${batchNum}/${totalBatches}: indexed ${result.indexed}, errors ${result.errors}`,
      );

      // ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸°
      if (i + ES_REINDEX_BATCH_SIZE < items.length) {
        await new Promise((resolve) => setTimeout(resolve, 300));
      }
    }

    console.log(
      `âœ“ Elasticsearch reindexing complete: ${totalIndexed} indexed, ${totalErrors} errors\n`,
    );
  } catch (error) {
    console.error(
      `âœ— Elasticsearch reindexing failed for ${tableName}:`,
      error.message,
    );
    // ì‹¤íŒ¨í•´ë„ í¬ë¡¤ë§ì€ ê³„ì† ì§„í–‰
  }
}

async function crawlAll() {
  if (isCrawling) {
    throw new Error("already crawling");
  } else {
    try {
      const [existingItems] = await pool.query(
        "SELECT item_id, auc_num FROM crawled_items",
      );

      // Config ê¸°ë°˜ìœ¼ë¡œ ê¸°ì¡´ ì•„ì´í…œ ID ê·¸ë£¹í™”
      const existingIdsByAuction = {};
      Object.keys(AUCTION_CONFIG).forEach((aucNum) => {
        existingIdsByAuction[aucNum] = new Set(
          existingItems
            .filter((item) => item.auc_num == aucNum)
            .map((item) => item.item_id),
        );
      });

      isCrawling = true;

      // í™œì„±í™”ëœ ëª¨ë“  ê²½ë§¤ì‚¬ í¬ë¡¤ë§ (ì§ë ¬)
      const allItems = [];
      for (const [aucNum, config] of Object.entries(AUCTION_CONFIG)) {
        if (config.enabled && config.crawler) {
          try {
            const items = await config.crawler.crawlAllItems(
              existingIdsByAuction[aucNum],
            );
            if (items && items.length > 0) {
              allItems.push(...items);
            }
          } catch (error) {
            console.error(`[${config.name}] Crawl failed:`, error);
          }
        }
      }

      await DBManager.saveItems(allItems, "crawled_items");

      // existing ì•„ì´í…œ ì—…ë°ì´íŠ¸ (title ì—†ëŠ” ì•„ì´í…œ)
      const itemsToUpdate = allItems.filter(
        (item) => !item.title && item.item_id,
      );
      if (itemsToUpdate.length > 0) {
        await DBManager.updateItems(itemsToUpdate, "crawled_items");
      }

      await DBManager.deleteItemsWithout(
        allItems.map((item) => item.item_id),
        "crawled_items",
      );
      await DBManager.cleanupUnusedImages("products");
      await syncAllData();

      // Elasticsearch ì „ì²´ ì¬ì¸ë±ì‹±
      await reindexElasticsearch("crawled_items");
    } catch (error) {
      throw error;
    } finally {
      isCrawling = false;
      await loginAll();
    }
  }
}

async function crawlAllValues(options = {}) {
  const { aucNums = [], months = [] } = options;

  // ì„ íƒëœ í¬ë¡¤ëŸ¬ê°€ ì—†ìœ¼ë©´ ëª¨ë‘ ì‹¤í–‰
  const runAll = aucNums.length === 0;

  // ê° í¬ë¡¤ëŸ¬ ì‹¤í–‰ ì—¬ë¶€ ê²°ì •
  const runEcoAuc = runAll || aucNums.includes(1);
  const runBrandAuc = runAll || aucNums.includes(2);
  const runStarAuc = runAll || aucNums.includes(3);
  const runMekikiAuc = runAll || aucNums.includes(4);

  // ê° í¬ë¡¤ëŸ¬ë³„ ê°œì›” ìˆ˜ ë§¤í•‘ (ê¸°ë³¸ê°’: 1ê°œì›”)
  const monthsMap = {
    1: 1, // EcoAuc
    2: 1, // BrandAuc
    3: 1, // StarAuc
    4: 1, // MekikiAuc
  };

  // ì…ë ¥ë°›ì€ months ë°°ì—´ì´ ìˆìœ¼ë©´, aucNumsì™€ ë§¤í•‘í•˜ì—¬ ì„¤ì •
  if (aucNums.length > 0 && months.length > 0) {
    for (let i = 0; i < Math.min(aucNums.length, months.length); i++) {
      monthsMap[aucNums[i]] = months[i];
    }
  }

  if (isValueCrawling) {
    throw new Error("Value crawling already in progress");
  }

  try {
    isValueCrawling = true;
    const startTime = Date.now();

    console.log("Starting value crawling with options:", {
      aucNums: aucNums.length > 0 ? aucNums : "all",
      months: monthsMap,
      runEcoAuc,
      runBrandAuc,
      runStarAuc,
      runMekikiAuc,
    });

    // DBì—ì„œ ê¸°ì¡´ ì•„ì´í…œ ID ì¡°íšŒ
    const [existingItems] = await pool.query(
      "SELECT item_id, auc_num FROM values_items",
    );

    // auc_numë³„ë¡œ ê¸°ì¡´ ì•„ì´í…œ ID ê·¸ë£¹í™”
    const existingIdsByAuction = {
      1: new Set(
        existingItems
          .filter((item) => item.auc_num == 1)
          .map((item) => item.item_id),
      ),
      2: new Set(
        existingItems
          .filter((item) => item.auc_num == 2)
          .map((item) => item.item_id),
      ),
      3: new Set(
        existingItems
          .filter((item) => item.auc_num == 3)
          .map((item) => item.item_id),
      ),
      4: new Set(
        existingItems
          .filter((item) => item.auc_num == 4)
          .map((item) => item.item_id),
      ),
    };

    // í¬ë¡¤ëŸ¬ë³„ ì„¤ì •
    const crawlerConfigs = [
      {
        enabled: runEcoAuc,
        aucNum: 1,
        name: "EcoAuc",
        crawler: ecoAucValueCrawler,
        months: monthsMap[1],
        folderName: "values",
        priority: 3,
        cropType: null,
        existingIds: existingIdsByAuction[1],
      },
      {
        enabled: runBrandAuc,
        aucNum: 2,
        name: "BrandAuc",
        crawler: brandAucValueCrawler,
        months: monthsMap[2],
        folderName: "values",
        priority: 3,
        cropType: "brand",
        existingIds: existingIdsByAuction[2],
      },
      {
        enabled: runStarAuc,
        aucNum: 3,
        name: "StarAuc",
        crawler: starAucValueCrawler,
        months: monthsMap[3],
        folderName: "values",
        priority: 3,
        cropType: null,
        existingIds: existingIdsByAuction[3],
      },
      {
        enabled: runMekikiAuc,
        aucNum: 4,
        name: "MekikiAuc",
        crawler: mekikiAucValueCrawler,
        months: monthsMap[4],
        folderName: "values",
        priority: 3,
        cropType: null,
        existingIds: existingIdsByAuction[4],
      },
    ];

    const results = {};

    // ê° í¬ë¡¤ëŸ¬ ìˆœì°¨ ì‹¤í–‰
    for (const config of crawlerConfigs) {
      if (!config.enabled) {
        results[config.name] = 0;
        continue;
      }

      console.log(`\n${"=".repeat(60)}`);
      console.log(
        `Starting ${config.name} value crawler for ${config.months} months`,
      );
      console.log("=".repeat(60));

      try {
        // 1. ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
        const metadata = await config.crawler.getStreamingMetadata(
          config.months,
        );
        console.log(`Total chunks to process: ${metadata.totalChunks}`);

        let processedChunks = 0;
        let totalItems = 0;

        // 2. ì²­í¬ë³„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
        for (const chunk of metadata.chunks) {
          processedChunks++;
          console.log(
            `\n[${processedChunks}/${metadata.totalChunks}] Processing chunk...`,
          );

          const chunkItems = await processChunk(
            config.crawler,
            chunk,
            config.existingIds,
            {
              folderName: config.folderName,
              priority: config.priority,
              cropType: config.cropType,
              tableName: "values_items",
            },
          );

          totalItems += chunkItems;
        }

        results[config.name] = totalItems;
        console.log(
          `\n${config.name} completed: ${totalItems} items processed`,
        );
      } catch (error) {
        console.error(`${config.name} crawling failed:`, error.message);
        results[config.name] = 0;
      }
    }

    // 3. ìµœì¢… ì •ë¦¬
    console.log("\n=== Finalizing value crawling ===");
    await DBManager.cleanupOldValueItems(999);
    await processBidsAfterCrawl();
    console.log("Cleanup completed");

    const endTime = Date.now();
    const totalCount = Object.values(results).reduce(
      (sum, count) => sum + count,
      0,
    );

    console.log(
      `\nValue crawling completed in ${formatExecutionTime(
        endTime - startTime,
      )}`,
    );

    // Elasticsearch ì „ì²´ ì¬ì¸ë±ì‹±
    await reindexElasticsearch("values_items");

    return {
      settings: {
        aucNums: aucNums.length > 0 ? aucNums : [1, 2, 3, 4],
        months: monthsMap,
      },
      results: {
        ecoAucCount: results.EcoAuc || 0,
        brandAucCount: results.BrandAuc || 0,
        starAucCount: results.StarAuc || 0,
        mekikiAucCount: results.MekikiAuc || 0,
        totalCount: totalCount,
      },
    };
  } catch (error) {
    console.error("Value crawling failed:", error);
    throw error;
  } finally {
    isValueCrawling = false;
  }
}

/**
 * ë‹¨ì¼ ì²­í¬ ì²˜ë¦¬: í¬ë¡¤ë§ â†’ ì´ë¯¸ì§€ â†’ DB â†’ S3
 */
async function processChunk(crawler, chunk, existingIds, options) {
  const { folderName, priority, cropType, tableName } = options;

  console.log(`\n=== Processing chunk ${chunk.startPage}-${chunk.endPage} ===`);
  if (chunk.categoryId) {
    console.log(`Category: ${chunk.categoryId}`);
  } else if (chunk.eventId) {
    console.log(`Event: ${chunk.eventId} (${chunk.eventTitle})`);
  } else if (chunk.bidType) {
    console.log(`Bid Type: ${chunk.bidType}`);
  }

  try {
    // Step 1: í¬ë¡¤ë§ (ë³‘ë ¬)
    const items = await crawler.crawlChunkPages(chunk, existingIds);

    if (items.length === 0) {
      console.log("No items in chunk, skipping");
      return 0;
    }

    console.log(`Crawled ${items.length} items`);

    // Step 2: ë¡œì»¬ ì´ë¯¸ì§€ ì €ì¥
    const itemsWithLocalImages = await processImagesInChunks(
      items,
      folderName,
      priority,
      cropType,
    );

    console.log(`Processed images for ${itemsWithLocalImages.length} items`);

    // Step 3: DB ì €ì¥
    await DBManager.saveItems(itemsWithLocalImages, tableName);
    console.log("Saved to DB");

    // Step 4: S3 ë§ˆì´ê·¸ë ˆì´ì…˜ (valuesë§Œ)
    if (folderName === "values") {
      await migrateChunkToS3(itemsWithLocalImages);
      console.log("Migrated to S3 and cleaned local files");
    }

    // ë©”ëª¨ë¦¬ ì •ë¦¬
    const itemCount = itemsWithLocalImages.length;
    items.length = 0;
    itemsWithLocalImages.length = 0;

    return itemCount;
  } catch (error) {
    console.error(`Chunk processing failed:`, error.message);
    // ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    return 0;
  }
}

/**
 * ì²­í¬ ë‹¨ìœ„ S3 ë§ˆì´ê·¸ë ˆì´ì…˜
 */
async function migrateChunkToS3(items) {
  const { ValuesImageMigration } = require("../utils/s3Migration");
  const migration = new ValuesImageMigration();

  try {
    const result = await migration.processItemsBatch(items);

    // ë¡œì»¬ íŒŒì¼ ì¦‰ì‹œ ì‚­ì œ
    await migration.cleanupLocalFiles(result.success);

    return result;
  } catch (error) {
    console.error("S3 migration failed:", error.message);
    // ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
    return { success: [], failed: items };
  }
}

// ê°œë³„ ê²½ë§¤ì‚¬ ì—…ë°ì´íŠ¸ í¬ë¡¤ë§
async function crawlUpdateForAuction(aucNum) {
  const config = AUCTION_CONFIG[aucNum];

  if (!config || !config.enabled) {
    console.log(`Auction ${aucNum} is not enabled`);
    return null;
  }

  // ì „ì—­ í¬ë¡¤ë§ ì²´í¬
  if (isCrawling || isValueCrawling) {
    throw new Error("Another global crawling in progress");
  }

  // í•´ë‹¹ ê²½ë§¤ì‚¬ í¬ë¡¤ë§ ì²´í¬
  if (crawlingStatus.update[aucNum]) {
    throw new Error(`Auction ${config.name} update already in progress`);
  }

  crawlingStatus.update[aucNum] = true;
  const startTime = Date.now();
  console.log(
    `[${config.name}] Starting update crawl at ${new Date().toISOString()}`,
  );

  try {
    // í¬ë¡¤ë§ ì‹¤í–‰
    let updates = await config.crawler.crawlUpdates();
    if (!updates) updates = [];

    if (updates.length === 0) {
      console.log(`[${config.name}] No updates found`);
      return {
        aucNum,
        name: config.name,
        count: 0,
        changedItemsCount: 0,
        executionTime: formatExecutionTime(Date.now() - startTime),
      };
    }

    // DBì—ì„œ ê¸°ì¡´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    const itemIds = updates.map((item) => item.item_id);
    const [existingItems] = await pool.query(
      "SELECT item_id, scheduled_date, starting_price FROM crawled_items WHERE item_id IN (?) AND bid_type = 'direct' AND auc_num = ?",
      [itemIds, aucNum],
    );

    // ë³€ê²½ëœ ì•„ì´í…œ í•„í„°ë§
    const changedItems = updates.filter((newItem) => {
      const existingItem = existingItems.find(
        (item) => item.item_id === newItem.item_id,
      );

      if (!existingItem) {
        return false;
      }

      let dateChanged = false;
      if (newItem.scheduled_date) {
        const newDate = new Date(newItem.scheduled_date);
        const existingDate = new Date(existingItem.scheduled_date);
        dateChanged = newDate.getTime() !== existingDate.getTime();
      }

      let priceChanged = false;
      if (newItem.starting_price) {
        const newPrice = parseFloat(newItem.starting_price) || 0;
        const existingPrice = parseFloat(existingItem.starting_price) || 0;
        priceChanged = Math.abs(newPrice - existingPrice) > 0.01;
      }

      return dateChanged || priceChanged;
    });

    // ë³€ê²½ëœ ì•„ì´í…œì´ ìˆìœ¼ë©´ DB ì—…ë°ì´íŠ¸ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
    if (changedItems.length > 0) {
      console.log(
        `[${config.name}] Found ${changedItems.length} changed items`,
      );
      DBManager.updateItems(changedItems, "crawled_items").then(() => {
        processChangedBids(changedItems).then(() => {
          notifyClientsOfChanges(changedItems);
        });
      });
    }

    const executionTime = Date.now() - startTime;
    console.log(
      `[${config.name}] Update crawl completed in ${formatExecutionTime(
        executionTime,
      )}`,
    );

    return {
      aucNum,
      name: config.name,
      count: updates.length,
      changedItemsCount: changedItems.length,
      executionTime: formatExecutionTime(executionTime),
    };
  } catch (error) {
    console.error(`[${config.name}] Update crawl failed:`, error);
    throw error;
  } finally {
    crawlingStatus.update[aucNum] = false;
  }
}

// ê°œë³„ ê²½ë§¤ì‚¬ ID ê¸°ë°˜ ì—…ë°ì´íŠ¸ í¬ë¡¤ë§
async function crawlUpdateWithIdForAuction(aucNum, itemIds, originalItems) {
  const config = AUCTION_CONFIG[aucNum];

  if (!config || !config.enabled) {
    console.log(`Auction ${aucNum} is not enabled`);
    return null;
  }

  // ì „ì—­ í¬ë¡¤ë§ ì²´í¬
  if (isCrawling || isValueCrawling) {
    throw new Error("Another global crawling in progress");
  }

  // í•´ë‹¹ ê²½ë§¤ì‚¬ í¬ë¡¤ë§ ì²´í¬
  if (crawlingStatus.updateWithId[aucNum]) {
    throw new Error(`Auction ${config.name} updateWithId already in progress`);
  }

  if (!itemIds || itemIds.length === 0) {
    return {
      aucNum,
      name: config.name,
      count: 0,
      changedItemsCount: 0,
    };
  }

  crawlingStatus.updateWithId[aucNum] = true;
  const startTime = Date.now();
  console.log(
    `[${config.name}] Starting updateWithId for ${itemIds.length} items`,
  );

  try {
    // í¬ë¡¤ë§ ì‹¤í–‰
    let updates = await config.crawler.crawlUpdateWithIds(itemIds);
    if (!updates) updates = [];

    // ë³€ê²½ëœ í•­ëª© í•„í„°ë§
    const changedItems = updates.filter((newItem) => {
      const originalItem = originalItems[newItem.item_id];
      if (!originalItem) {
        return false;
      }

      // ë‚ ì§œ ë³€ê²½ í™•ì¸
      let dateChanged = false;
      if (newItem.scheduled_date) {
        const newDate = new Date(newItem.scheduled_date);
        const originalDate = new Date(originalItem.scheduled_date);
        dateChanged = newDate.getTime() !== originalDate.getTime();
      }

      // ê°€ê²© ë³€ê²½ í™•ì¸
      let priceChanged = false;
      if (newItem.starting_price) {
        const newPrice = parseFloat(newItem.starting_price) || 0;
        const originalPrice = parseFloat(originalItem.starting_price) || 0;
        priceChanged = Math.abs(newPrice - originalPrice) > 0.01;
      }

      return dateChanged || priceChanged;
    });

    // ë³€ê²½ëœ ì•„ì´í…œì´ ìˆìœ¼ë©´ DB ì—…ë°ì´íŠ¸ ë¹„ë™ê¸°ë¡œ ì‹¤í–‰
    if (changedItems.length > 0) {
      console.log(
        `[${config.name}] Found ${changedItems.length} changed items`,
      );
      DBManager.updateItems(changedItems, "crawled_items").then(() => {
        processChangedBids(changedItems).then(() => {
          notifyClientsOfChanges(changedItems);
        });
      });
    }

    const executionTime = Date.now() - startTime;
    console.log(
      `[${config.name}] UpdateWithId completed in ${formatExecutionTime(
        executionTime,
      )}`,
    );

    return {
      aucNum,
      name: config.name,
      count: updates.length,
      changedItemsCount: changedItems.length,
      executionTime: formatExecutionTime(executionTime),
    };
  } catch (error) {
    console.error(`[${config.name}] UpdateWithId failed:`, error);
    throw error;
  } finally {
    crawlingStatus.updateWithId[aucNum] = false;
  }
}

// live_bidsì˜ winning_price ìë™ ì—…ë°ì´íŠ¸ ì²˜ë¦¬
async function processBidsAfterCrawl() {
  const conn = await pool.getConnection();
  try {
    await conn.beginTransaction();

    // 1. ì´ë¯¸ cancelled ìƒíƒœì¸ ì…ì°°ë“¤ì˜ winning_price ì—…ë°ì´íŠ¸
    await conn.query(
      `UPDATE live_bids lb
       JOIN values_items vi ON lb.item_id = vi.item_id
       SET lb.winning_price = vi.final_price
       WHERE lb.status = 'cancelled' AND (lb.winning_price IS NULL OR lb.winning_price < vi.final_price)`,
    );

    // ìš°ì„  final_priceë¡œ ì—…ë°ì´íŠ¸
    await conn.query(
      `UPDATE direct_bids db
       JOIN values_items vi ON db.item_id = vi.item_id
       SET db.winning_price = vi.final_price
       WHERE db.status = 'cancelled' AND (db.winning_price IS NULL OR db.winning_price < vi.final_price)`,
    );

    // 2. final ìƒíƒœì´ë©´ì„œ final_price < vi.final_priceì¸ ì…ì°°ë“¤ ì¡°íšŒ
    const [bidsToCancel] = await conn.query(
      `SELECT lb.id, lb.user_id, vi.final_price, u.account_type
       FROM live_bids lb
       JOIN values_items vi ON lb.item_id = vi.item_id
       JOIN user_accounts u ON lb.user_id = u.user_id
       WHERE lb.status = 'final' AND lb.final_price < vi.final_price`,
    );

    if (bidsToCancel.length > 0) {
      const bidIds = bidsToCancel.map((bid) => bid.id);
      const placeholders = bidIds.map(() => "?").join(",");

      // ê° ì…ì°°ì— ëŒ€í•´ ì˜ˆì¹˜ê¸ˆ/í•œë„ ë³µêµ¬
      for (const bid of bidsToCancel) {
        const deductAmount = await getBidDeductAmount(conn, bid.id, "live_bid");

        if (deductAmount > 0) {
          if (bid.account_type === "individual") {
            await refundDeposit(
              conn,
              bid.user_id,
              deductAmount,
              "live_bid",
              bid.id,
              "ë‚™ì°°ê°€ ì´ˆê³¼ë¡œ ì¸í•œ ì·¨ì†Œ í™˜ë¶ˆ",
            );
          } else {
            await refundLimit(
              conn,
              bid.user_id,
              deductAmount,
              "live_bid",
              bid.id,
              "ë‚™ì°°ê°€ ì´ˆê³¼ë¡œ ì¸í•œ ì·¨ì†Œ í™˜ë¶ˆ",
            );
          }
        }
      }

      // statusë¥¼ cancelledë¡œ ë³€ê²½í•˜ê³  winning_price ì„¤ì •
      await conn.query(
        `UPDATE live_bids lb
         JOIN values_items vi ON lb.item_id = vi.item_id
         SET lb.status = 'cancelled', lb.winning_price = vi.final_price
         WHERE lb.id IN (${placeholders})`,
        bidIds,
      );
    }

    await conn.commit();
    console.log("Winning_price updated successfully after crawl");
  } catch (error) {
    await conn.rollback();
    console.error("Error processing live bids after crawl:", error);
  } finally {
    conn.release();
  }
}

// ê°€ê²© ë³€ê²½ì— ë”°ë¥¸ ì…ì°° ì·¨ì†Œ ì²˜ë¦¬
async function processChangedBids(changedItems) {
  // ì²˜ë¦¬í•  ê²ƒì´ ì—†ìœ¼ë©´ ë¹ ë¥´ê²Œ ì¢…ë£Œ
  if (!changedItems || changedItems.length === 0) return;

  // ê°€ê²©ì´ ë³€ê²½ëœ ì•„ì´í…œë§Œ í•„í„°ë§
  const priceChangedItems = changedItems.filter((item) => item.starting_price);
  if (priceChangedItems.length === 0) return;

  const conn = await pool.getConnection();
  try {
    await conn.beginTransaction();

    // ê´€ë ¨ëœ ëª¨ë“  active ì…ì°° ì¡°íšŒ (account_type í¬í•¨)
    const [activeBids] = await conn.query(
      "SELECT db.id, db.item_id, db.user_id, db.current_price, ci.starting_price, u.account_type " +
        "FROM direct_bids db " +
        "JOIN crawled_items ci ON db.item_id = ci.item_id " +
        "JOIN user_accounts u ON db.user_id = u.user_id " +
        "WHERE db.current_price < ci.starting_price AND db.status = 'active'",
    );

    // ì·¨ì†Œí•´ì•¼ í•  ì…ì°° ID ì°¾ê¸°
    const bidsToCancel = activeBids.map((bid) => bid.id);

    let cancelledBidsData = [];
    if (bidsToCancel.length > 0) {
      [cancelledBidsData] = await conn.query(
        `SELECT db.user_id, i.title 
        FROM direct_bids db 
        JOIN crawled_items i ON db.item_id = i.item_id 
        WHERE db.id IN (${bidsToCancel.map(() => "?").join(",")})`,
        bidsToCancel,
      );
    }

    // ì˜ˆì¹˜ê¸ˆ/í•œë„ ë³µêµ¬ ë° ì·¨ì†Œ ì²˜ë¦¬ - 100ê°œì”© ë°°ì¹˜ ì²˜ë¦¬
    for (let i = 0; i < activeBids.length; i += 100) {
      const batch = activeBids.slice(i, i + 100);

      if (batch.length > 0) {
        // ê° ì…ì°°ì— ëŒ€í•´ ì˜ˆì¹˜ê¸ˆ/í•œë„ ë³µêµ¬
        for (const bid of batch) {
          const deductAmount = await getBidDeductAmount(
            conn,
            bid.id,
            "direct_bid",
          );

          if (deductAmount > 0) {
            if (bid.account_type === "individual") {
              await refundDeposit(
                conn,
                bid.user_id,
                deductAmount,
                "direct_bid",
                bid.id,
                "ê°€ê²© ë³€ê²½ìœ¼ë¡œ ì¸í•œ ì·¨ì†Œ í™˜ë¶ˆ",
              );
            } else {
              await refundLimit(
                conn,
                bid.user_id,
                deductAmount,
                "direct_bid",
                bid.id,
                "ê°€ê²© ë³€ê²½ìœ¼ë¡œ ì¸í•œ ì·¨ì†Œ í™˜ë¶ˆ",
              );
            }
          }
        }

        // ì…ì°° ìƒíƒœ ë³€ê²½
        const batchIds = batch.map((b) => b.id);
        await conn.query(
          "UPDATE direct_bids SET status = 'cancelled' WHERE id IN (?) AND status = 'active'",
          [batchIds],
        );

        console.log(
          `Cancelled batch ${Math.floor(i / 100) + 1}: ${
            batch.length
          } bids due to price changes (deposits/limits refunded)`,
        );
      }
    }

    await conn.commit();

    // ì·¨ì†Œëœ ì…ì°°ìë“¤ì—ê²Œ ì•Œë¦¼ ë°œì†¡ (ë¹„ë™ê¸°)
    if (cancelledBidsData.length > 0) {
      sendHigherBidAlerts(cancelledBidsData);
    }

    console.log(
      `Total cancelled due to price changes: ${bidsToCancel.length} bids`,
    );
  } catch (error) {
    await conn.rollback();
    console.error("Error cancelling bids:", error);
  } finally {
    conn.release();
  }
}

// ì¸ë³´ì´ìŠ¤ í¬ë¡¤ë§ í•¨ìˆ˜
async function crawlAllInvoices() {
  try {
    console.log(`Starting invoice crawl at ${new Date().toISOString()}`);
    const startTime = Date.now();

    // Config ê¸°ë°˜ìœ¼ë¡œ í™œì„±í™”ëœ í¬ë¡¤ëŸ¬ì—ì„œ ì¸ë³´ì´ìŠ¤ í¬ë¡¤ë§
    const invoicePromises = [];
    const resultsByAucNum = {};

    Object.entries(AUCTION_CONFIG).forEach(([aucNum, config]) => {
      if (config.enabled && config.crawler && config.crawler.crawlInvoices) {
        invoicePromises.push(
          config.crawler
            .crawlInvoices()
            .then((invoices) => {
              resultsByAucNum[aucNum] = invoices || [];
              return invoices || [];
            })
            .catch((error) => {
              console.error(`[${config.name}] Invoice crawl failed:`, error);
              resultsByAucNum[aucNum] = [];
              return [];
            }),
        );
      }
    });

    const results = await Promise.all(invoicePromises);
    const allInvoices = results.flat();

    // DBì— ì €ì¥
    await DBManager.saveItems(allInvoices, "invoices");

    const endTime = Date.now();
    const executionTime = endTime - startTime;

    console.log(
      `Invoice crawl completed in ${formatExecutionTime(executionTime)}`,
    );

    return {
      ecoAucCount: resultsByAucNum[1]?.length || 0,
      brandAucCount: resultsByAucNum[2]?.length || 0,
      starAucCount: resultsByAucNum[3]?.length || 0,
      mekikiAucCount: resultsByAucNum[4]?.length || 0,
      totalCount: allInvoices.length,
      executionTime: formatExecutionTime(executionTime),
    };
  } catch (error) {
    console.error("Invoice crawl failed:", error);
    throw error;
  }
}

// ì‹¤í–‰ ì‹œê°„ í¬ë§·íŒ… í•¨ìˆ˜
function formatExecutionTime(milliseconds) {
  const seconds = Math.floor(milliseconds / 1000);
  const minutes = Math.floor(seconds / 60);
  const hours = Math.floor(minutes / 60);

  const remainingMinutes = minutes % 60;
  const remainingSeconds = seconds % 60;

  return `${hours}h ${remainingMinutes}m ${remainingSeconds}s`;
}

router.get("/crawl", isAdmin, async (req, res) => {
  try {
    await crawlAll();

    res.json({
      message: "Crawling and image processing completed successfully",
    });
  } catch (error) {
    console.error("Crawling error:", error);
    res.status(500).json({ message: "Error during crawling" });
  }
});

router.get("/crawl-values", isAdmin, async (req, res) => {
  try {
    // auc_num íŒŒë¼ë¯¸í„° íŒŒì‹± (auc_num=1,2,3 í˜•íƒœë¡œ ë°›ìŒ)
    const aucNums = req.query.auc_num
      ? req.query.auc_num.split(",").map((num) => parseInt(num.trim()))
      : [];

    // months íŒŒë¼ë¯¸í„° íŒŒì‹± (months=3,6,12 í˜•íƒœë¡œ ë°›ìŒ)
    const monthsInput = req.query.months
      ? req.query.months.split(",").map((m) => parseInt(m.trim()))
      : [];

    // ê²°ê³¼ ë° ì‹¤í–‰ ì •ë³´
    const result = await crawlAllValues({
      aucNums,
      months: monthsInput,
    });

    res.json({
      message: "Value crawling completed successfully",
      result,
    });
  } catch (error) {
    console.error("Value crawling error:", error);
    res
      .status(500)
      .json({ message: "Error during value crawling", error: error.message });
  }
});

router.get("/crawl-status", isAdmin, (req, res) => {
  try {
    // ê° ê²½ë§¤ì‚¬ë³„ ìŠ¤ì¼€ì¤„ëŸ¬ ìƒíƒœ
    const auctionStatuses = {};
    Object.keys(AUCTION_CONFIG).forEach((aucNum) => {
      const config = AUCTION_CONFIG[aucNum];
      auctionStatuses[aucNum] = {
        name: config.name,
        enabled: config.enabled,
        update: {
          crawling: crawlingStatus.update[aucNum] || false,
          scheduler: updateSchedulers[aucNum]?.getStatus() || null,
        },
        updateWithId: {
          crawling: crawlingStatus.updateWithId[aucNum] || false,
          scheduler: updateWithIdSchedulers[aucNum]?.getStatus() || null,
        },
      };
    });

    res.json({
      global: {
        isCrawling,
        isValueCrawling,
        isUpdateCrawling,
        isUpdateCrawlingWithId,
      },
      auctions: auctionStatuses,
    });
  } catch (error) {
    console.error("Crawling status error:", error);
    res.status(500).json({ message: "Error getting crawling status" });
  }
});

// ì¸ë³´ì´ìŠ¤ í¬ë¡¤ë§ ë¼ìš°íŒ… ì¶”ê°€
router.get("/crawl-invoices", isAdmin, async (req, res) => {
  try {
    const result = await crawlAllInvoices();

    res.json({
      message: "Invoice crawling completed successfully",
      stats: result,
    });
  } catch (error) {
    console.error("Invoice crawling error:", error);
    res.status(500).json({
      message: "Error during invoice crawling",
      error: error.message,
    });
  }
});

router.post("/migrate-to-s3", isAdmin, async (req, res) => {
  try {
    const migration = new ValuesImageMigration();
    const stats = await migration.migrate();

    res.json({
      message: "Migration completed",
      stats,
    });
  } catch (error) {
    console.error("Migration error:", error);
    res.status(500).json({
      message: "Migration failed",
      error: error.message,
    });
  }
});

router.get("/migration-status", isAdmin, async (req, res) => {
  try {
    const migration = new ValuesImageMigration();
    const remainingCount = await migration.getRemainingCount();

    res.json({
      remainingCount,
      estimatedBatches: Math.ceil(remainingCount / 50),
    });
  } catch (error) {
    console.error("Status check error:", error);
    res.status(500).json({
      message: "Status check failed",
      error: error.message,
    });
  }
});

const scheduleCrawling = async () => {
  const settings = await getAdminSettings();

  console.log(`Crawling schedule: ${settings.crawlSchedule}`);

  if (settings && settings.crawlSchedule) {
    const [hours, minutes] = settings.crawlSchedule.split(":");
    cron.schedule(
      `${minutes} ${hours} * * *`,
      async () => {
        console.log("Running scheduled crawling task");
        try {
          // ê¸°ë³¸ ìƒí’ˆ í¬ë¡¤ë§ ì‹¤í–‰
          await crawlAll();

          // ì‹œì„¸í‘œ í¬ë¡¤ë§ë„ ì‹¤í–‰
          console.log("Running value crawling");
          await crawlAllValues();

          // ì¸ë³´ì´ìŠ¤ í¬ë¡¤ë§ë„ ì‹¤í–‰
          console.log("Running invoice crawling");
          await crawlAllInvoices();

          console.log("Scheduled crawling completed successfully");
        } catch (error) {
          console.error("Scheduled crawling error:", error);
        }
      },
      {
        scheduled: true,
        timezone: "Asia/Seoul",
      },
    );
  }
};

// ê°œë³„ ê²½ë§¤ì‚¬ë³„ ì—…ë°ì´íŠ¸ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë§
const scheduleUpdateCrawlingForAuction = (aucNum) => {
  const config = AUCTION_CONFIG[aucNum];
  const scheduler = updateSchedulers[aucNum];

  if (!config || !scheduler || config.updateInterval === 0) {
    console.log(`Update crawling disabled for auction ${aucNum}`);
    return null;
  }

  let timeoutId;

  const runUpdateCrawl = async () => {
    console.log(
      `[${config.name}] Running update crawl (interval: ${
        scheduler.getStatus().current
      }s)`,
    );

    try {
      if (!isCrawling && !isValueCrawling) {
        const result = await crawlUpdateForAuction(aucNum);

        if (result) {
          const nextInterval = scheduler.next(result.changedItemsCount);
          console.log(
            `[${config.name}] Completed - changed: ${result.changedItemsCount}, next: ${nextInterval}s`,
          );
          timeoutId = setTimeout(runUpdateCrawl, nextInterval * 1000);
        } else {
          timeoutId = setTimeout(runUpdateCrawl, scheduler.base * 1000);
        }
      } else {
        console.log(`[${config.name}] Skipped - global crawling active`);
        timeoutId = setTimeout(runUpdateCrawl, scheduler.current * 1000);
      }
    } catch (error) {
      console.error(`[${config.name}] Error:`, error.message);
      timeoutId = setTimeout(runUpdateCrawl, scheduler.base * 1000);
    }
  };

  // ì²« ë”œë ˆì´ í›„ ì‹¤í–‰
  timeoutId = setTimeout(runUpdateCrawl, scheduler.base * 1000);

  return () => clearTimeout(timeoutId);
};

// ëª¨ë“  í™œì„±í™”ëœ ê²½ë§¤ì‚¬ì˜ ì—…ë°ì´íŠ¸ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë§
const scheduleUpdateCrawling = () => {
  const cancelFunctions = [];

  Object.keys(AUCTION_CONFIG).forEach((aucNum) => {
    if (AUCTION_CONFIG[aucNum].enabled) {
      const cancelFn = scheduleUpdateCrawlingForAuction(parseInt(aucNum));
      if (cancelFn) {
        cancelFunctions.push(cancelFn);
      }
    }
  });

  // ëª¨ë“  ìŠ¤ì¼€ì¤„ ì·¨ì†Œ í•¨ìˆ˜ ë°˜í™˜
  return () => {
    cancelFunctions.forEach((fn) => fn());
  };
};

// ê°œë³„ ê²½ë§¤ì‚¬ë³„ ID ê¸°ë°˜ ì—…ë°ì´íŠ¸ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë§
const scheduleUpdateCrawlingWithIdForAuction = (aucNum) => {
  const config = AUCTION_CONFIG[aucNum];
  const scheduler = updateWithIdSchedulers[aucNum];

  if (!config || !scheduler || config.updateWithIdInterval === 0) {
    console.log(`UpdateWithId crawling disabled for auction ${aucNum}`);
    return null;
  }

  let timeoutId;

  const runUpdateCrawlWithId = async () => {
    console.log(
      `[${config.name}] Running updateWithId (interval: ${
        scheduler.getStatus().current
      }s)`,
    );

    try {
      if (!isCrawling && !isValueCrawling) {
        // active bidsë¥¼ ì¡°íšŒí•˜ê³  í•´ë‹¹ ê²½ë§¤ì‚¬ ì•„ì´í…œë§Œ í•„í„°ë§
        const [activeBids] = await pool.query(
          `SELECT DISTINCT db.item_id, ci.scheduled_date, ci.starting_price
           FROM direct_bids db
           JOIN crawled_items ci ON db.item_id = ci.item_id
           WHERE ci.bid_type = 'direct' 
             AND ci.auc_num = ?
             AND db.status = 'active'
             AND ci.scheduled_date >= DATE_SUB(NOW(), INTERVAL 10 HOUR)`,
          [aucNum],
        );

        if (activeBids.length > 0) {
          const itemIds = activeBids.map((bid) => bid.item_id);
          const originalItems = {};
          activeBids.forEach((bid) => {
            originalItems[bid.item_id] = {
              item_id: bid.item_id,
              scheduled_date: bid.scheduled_date,
              starting_price: bid.starting_price,
            };
          });

          const result = await crawlUpdateWithIdForAuction(
            aucNum,
            itemIds,
            originalItems,
          );

          if (result) {
            const nextInterval = scheduler.next(result.changedItemsCount);
            console.log(
              `[${config.name}] Completed - changed: ${result.changedItemsCount}, next: ${nextInterval}s`,
            );
            timeoutId = setTimeout(runUpdateCrawlWithId, nextInterval * 1000);
          } else {
            timeoutId = setTimeout(runUpdateCrawlWithId, scheduler.base * 1000);
          }
        } else {
          const nextInterval = scheduler.next(0);
          timeoutId = setTimeout(runUpdateCrawlWithId, nextInterval * 1000);
        }
      } else {
        console.log(`[${config.name}] Skipped - global crawling active`);
        timeoutId = setTimeout(runUpdateCrawlWithId, scheduler.current * 1000);
      }
    } catch (error) {
      console.error(`[${config.name}] Error:`, error.message);
      timeoutId = setTimeout(runUpdateCrawlWithId, scheduler.base * 1000);
    }
  };

  // ì²« ë”œë ˆì´ í›„ ì‹¤í–‰
  timeoutId = setTimeout(runUpdateCrawlWithId, scheduler.base * 1000);

  return () => clearTimeout(timeoutId);
};

// ëª¨ë“  í™œì„±í™”ëœ ê²½ë§¤ì‚¬ì˜ ID ê¸°ë°˜ ì—…ë°ì´íŠ¸ í¬ë¡¤ë§ ìŠ¤ì¼€ì¤„ë§
const scheduleUpdateCrawlingWithId = () => {
  const cancelFunctions = [];

  Object.keys(AUCTION_CONFIG).forEach((aucNum) => {
    if (AUCTION_CONFIG[aucNum].enabled) {
      const cancelFn = scheduleUpdateCrawlingWithIdForAuction(parseInt(aucNum));
      if (cancelFn) {
        cancelFunctions.push(cancelFn);
      }
    }
  });

  // ëª¨ë“  ìŠ¤ì¼€ì¤„ ì·¨ì†Œ í•¨ìˆ˜ ë°˜í™˜
  return () => {
    cancelFunctions.forEach((fn) => fn());
  };
};

// Socket.IO ì´ˆê¸°í™” (server.jsì—ì„œ ë¶ˆëŸ¬ì˜´)
function initializeSocket(server) {
  const io = socketIO(server);

  // í´ë¼ì´ì–¸íŠ¸ ì—°ê²° ì´ë²¤íŠ¸
  io.on("connection", (socket) => {
    console.log("Client connected:", socket.id);

    socket.on("disconnect", () => {
      console.log("Client disconnected:", socket.id);
    });
  });

  return io;
}

// ì„œë²„ì—ì„œ ë°ì´í„° ë³€ê²½ ê°ì§€ ì‹œ ì•Œë¦¼ ì „ì†¡
async function notifyClientsOfChanges(changedItems) {
  if (!global.io || changedItems.length === 0) return;

  // ë³€ê²½ëœ ì•„ì´í…œ IDë§Œ ì „ì†¡
  const changedItemIds = changedItems.map((item) => item.item_id);
  global.io.emit("data-updated", {
    itemIds: changedItemIds,
    timestamp: new Date().toISOString(),
  });

  console.log(`Notified clients about ${changedItemIds.length} updated items`);
}

// product í™˜ê²½ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ì¶”ê°€
if (process.env.ENV === "development") {
  console.log("development env");
} else {
  console.log("product env");
  scheduleCrawling();
  scheduleUpdateCrawling();
  scheduleUpdateCrawlingWithId();
  loginAll();
}

module.exports = { router, initializeSocket, notifyClientsOfChanges };
